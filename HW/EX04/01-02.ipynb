{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average file compression rate: 0.08%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Get a list of all .jpg files in the current directory\n",
    "files = [f for f in os.listdir() if os.path.isfile(f) and f.endswith('.jpg')]\n",
    "\n",
    "# List to store individual compression rates\n",
    "compression_rates = []\n",
    "\n",
    "# Iterate over the .jpg files and compress them\n",
    "for file in files:\n",
    "    # Define the output path for the compressed file\n",
    "    output_path = file + \".zip\"\n",
    "    \n",
    "    try:\n",
    "        # Compress the file using zipfile with ZIP_DEFLATED mode for better compression\n",
    "        with zipfile.ZipFile(output_path, \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n",
    "            zip_file.write(file)\n",
    "\n",
    "        # Calculate the compression rate\n",
    "        original_size = os.path.getsize(file)\n",
    "        compressed_size = os.path.getsize(output_path)\n",
    "        compression_rate = (original_size - compressed_size) / original_size * 100\n",
    "        compression_rates.append(compression_rate)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}. Reason: {e}\")\n",
    "\n",
    "# Calculate the average compression rate for all .jpg files\n",
    "average_compression_rate = sum(compression_rates) / len(compression_rates)\n",
    "print(f\"Average file compression rate: {average_compression_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing with colormap size: 256\n",
      "Time taken: 00:00:32.836 for colormap size: 256\n",
      "\n",
      "Compressing with colormap size: 128\n",
      "Time taken: 00:00:15.561 for colormap size: 128\n",
      "\n",
      "Compressing with colormap size: 64\n",
      "Time taken: 00:00:07.715 for colormap size: 64\n",
      "\n",
      "Compressing with colormap size: 32\n",
      "Time taken: 00:00:03.691 for colormap size: 32\n",
      "\n",
      "Compressing with colormap size: 16\n",
      "Time taken: 00:00:01.655 for colormap size: 16\n",
      "\n",
      "Compressing with colormap size: 8\n",
      "Time taken: 00:00:00.857 for colormap size: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "import heapq\n",
    "import struct\n",
    "from typing import Tuple, Dict\n",
    "import time\n",
    "\n",
    "class HuffmanTreeNode:\n",
    "    def __init__(self, value=None, frequency=None):\n",
    "        self.value = value\n",
    "        self.frequency = frequency\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.frequency < other.frequency\n",
    "\n",
    "def image_to_data(image: np.ndarray) -> np.ndarray:\n",
    "    return image.reshape((-1, 3))\n",
    "\n",
    "def quantize_image(image: np.ndarray, n_colors: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    data = image_to_data(image)\n",
    "    kmeans = KMeans(n_clusters=n_colors, n_init=10)\n",
    "    kmeans.fit(data)\n",
    "    new_data = kmeans.cluster_centers_[kmeans.labels_]\n",
    "    quantized_image = new_data.reshape(image.shape).astype(np.uint8)\n",
    "    return quantized_image, kmeans.cluster_centers_.astype(np.uint8)\n",
    "\n",
    "def to_indexed_image(image: np.ndarray, colormap: np.ndarray) -> np.ndarray:\n",
    "    data = image_to_data(image)\n",
    "    indexed_data = np.argmin(np.linalg.norm(colormap - data[:, np.newaxis], axis=2), axis=1)\n",
    "    return indexed_data.reshape(image.shape[:-1])\n",
    "\n",
    "def build_huffman_tree(frequency: Dict[int, int]) -> HuffmanTreeNode:\n",
    "    heap = [HuffmanTreeNode(value=char, frequency=freq) for char, freq in frequency.items()]\n",
    "    heapq.heapify(heap)\n",
    "    while len(heap) > 1:\n",
    "        left = heapq.heappop(heap)\n",
    "        right = heapq.heappop(heap)\n",
    "        merged_node = HuffmanTreeNode(frequency=left.frequency + right.frequency)\n",
    "        merged_node.left = left\n",
    "        merged_node.right = right\n",
    "        heapq.heappush(heap, merged_node)\n",
    "    return heap[0]\n",
    "\n",
    "def build_huffman_codes(tree: HuffmanTreeNode) -> Dict[int, str]:\n",
    "    codes = {}\n",
    "    def traverse(node, code=\"\"):\n",
    "        if node is None:\n",
    "            return\n",
    "        if node.value is not None:\n",
    "            codes[node.value] = code\n",
    "            return\n",
    "        traverse(node.left, code + \"0\")\n",
    "        traverse(node.right, code + \"1\")\n",
    "    traverse(tree)\n",
    "    return codes\n",
    "\n",
    "def huffman_compress(image: np.ndarray, colormap_size: int, output_file: str):\n",
    "    quantized_image, colormap = quantize_image(image, colormap_size)\n",
    "    indexed_image = to_indexed_image(quantized_image, colormap)\n",
    "    color_counts = dict(zip(*np.unique(indexed_image, return_counts=True)))\n",
    "    huffman_tree = build_huffman_tree(color_counts)\n",
    "    huffman_codes = build_huffman_codes(huffman_tree)\n",
    "    encoded_data = ''.join(huffman_codes[i] for i in indexed_image.ravel())\n",
    "    padded_encoded_data = encoded_data + '0' * (8 - len(encoded_data) % 8)\n",
    "    encoded_bytes = bytearray(int(padded_encoded_data[i:i+8], 2) for i in range(0, len(padded_encoded_data), 8))\n",
    "    \n",
    "    with open(output_file, 'wb') as file:\n",
    "        file.write(struct.pack('I', colormap_size))\n",
    "        \n",
    "        file.write(colormap.tobytes())\n",
    "        \n",
    "        file.write(struct.pack('I', len(indexed_image.ravel())))\n",
    "        \n",
    "        file.write(struct.pack('I', len(color_counts)))\n",
    "        for color, count in color_counts.items():\n",
    "            file.write(struct.pack('B', color))\n",
    "            file.write(struct.pack('I', count))\n",
    "        \n",
    "        file.write(encoded_bytes)\n",
    "\n",
    "def huffman_decompress(input_file: str, output_file: str):\n",
    "    with open(input_file, 'rb') as file:\n",
    "        colormap_size = struct.unpack('I', file.read(4))[0]\n",
    "        colormap = np.frombuffer(file.read(3 * colormap_size), dtype=np.uint8).reshape(-1, 3)\n",
    "        \n",
    "        original_length = struct.unpack('I', file.read(4))[0]\n",
    "        \n",
    "        num_colors = struct.unpack('I', file.read(4))[0]\n",
    "        color_counts = {}\n",
    "        for _ in range(num_colors):\n",
    "            color = struct.unpack('B', file.read(1))[0]\n",
    "            count = struct.unpack('I', file.read(4))[0]\n",
    "            color_counts[color] = count\n",
    "        \n",
    "        encoded_data = file.read()\n",
    "\n",
    "    huffman_tree = build_huffman_tree(color_counts)\n",
    "    huffman_codes = build_huffman_codes(huffman_tree)\n",
    "    decoded_data = []\n",
    "    current_code = \"\"\n",
    "    for byte in encoded_data:\n",
    "        current_code += bin(byte)[2:].rjust(8, '0')\n",
    "        while current_code:\n",
    "            found = False\n",
    "            for char, code in huffman_codes.items():\n",
    "                if current_code.startswith(code):\n",
    "                    decoded_data.append(char)\n",
    "                    current_code = current_code[len(code):]\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                break\n",
    "\n",
    "    indexed_image = np.array(decoded_data[:original_length], dtype=np.uint8).reshape(input_image.shape[:-1])\n",
    "    quantized_image = colormap[indexed_image]\n",
    "    cv2.imwrite(output_file, quantized_image)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_image_path = 'ZhongXinaSmaller.jpg'\n",
    "    compressed_file = 'compressed.hmc'\n",
    "\n",
    "    input_image = cv2.imread(input_image_path)\n",
    "    if input_image is None:\n",
    "        raise ValueError(f\"Could not open or find the image: {input_image_path}\")\n",
    "\n",
    "    colormap_sizes = [256, 128, 64, 32, 16, 8]\n",
    "    for colormap_size in colormap_sizes:\n",
    "        print(f\"Compressing with colormap size: {colormap_size}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        huffman_compress(input_image, colormap_size, compressed_file)\n",
    "        \n",
    "        decompressed_file = f'result_huffman_{colormap_size}.bmp'\n",
    "        \n",
    "        huffman_decompress(compressed_file, decompressed_file)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        hours, rem = divmod(elapsed_time, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        milliseconds = int((seconds % 1) * 1000)\n",
    "        print(f\"Time taken: {int(hours):02}:{int(minutes):02}:{int(seconds):02}.{milliseconds:03} for colormap size: {colormap_size}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
